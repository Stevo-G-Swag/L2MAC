# L2MAC 101

After this tutorial, you will be able to:

1. Use L2MAC to generate anything for your prompt task.
2. Run L2AMC to achieve the best results for your prompt task or generation idea.

## Using Standard L2MAC

The LLM Automatic Computer (L2MAC) framework can be used with default settings to generate an extensive text-based output in multiple files for your input prompt. Examples of tasks are given in the [examples folder](https://github.com/samholt/L2MAC/tree/master/examples), including examples for developing simple and complex codebases and an entire book. L2MAC has two general functions, `generate_codebase` and `generate_book`, which are both called the underlying function of [`run_l2mac`](https://github.com/samholt/L2MAC/blob/master/l2mac/core.py).

```python
from l2mac import generate_codebase

# run pip install pygame==2.1.2
codebase: dict = generate_codebase(
    "Create a beautiful, playable and simple snake game with pygame. Make the snake and food be aligned to the same 10-pixel grid.",
    steps=2,
)

print(codebase)  # it will print the codebase (repo) complete with all the files as a dictionary
```

This will create a new `workspace` folder in your local directory where you run this script, including all the files generated while running and when L2MAC has completed. The final output codebase will be within the newly generated folder in the local `workspace` directory, organized by the time and date as the sub-folder name.

## Producing the best outputs with L2MAC

To generate or produce the best output with the LLM Automatic Computer Framework (L2MAC), a few key parameters can be tuned that significantly impact the length and quality of the generated output; they are given in order of priority here:

### Adding more detail to the input prompt task

Creating a more detailed input prompt that describes the task and exactly what you would like L2MAC to generate will improve the output quality. As L2MAC is a framework that augments any LLM, [standard LLM prompting techniques and rules still apply](https://platform.openai.com/docs/guides/prompt-engineering/six-strategies-for-getting-better-results), including sufficient detail in the input prompt task. For example, to generate a complex large codebase, it helps to specify many detailed feature requirements, for example, see [generating a complex large codebase for a URL web application.](https://github.com/samholt/L2MAC/blob/master/examples/generate_codebase_complex_url_shortener_app.py).

### Increasing the number of steps

Increasing the number of steps within the prompt program plan that the LLM agent will execute usually improves the length and quality of the generated output. This can modified explicitly through the steps parameter; a valid default value is 10. Setting a value that is too large could mean many steps in the prompt program. Therefore, it could take longer to generate the final output. For example, you can set this to `steps=10`.

### Running tests for generated code

When generating code for a codebase, a user can optionally specify to run self-generated unit tests that are automatically generated alongside the codebase. This also includes running a static code analyzer to check for any invalid code, and any errors regarding either unit tests or invalid code are iterated with the LLM agent to fix them before continuing onto the next prompt-program instruction step to complete. For coding tasks this can significantly improve the quality of the output, especially for coding tasks that involve complex integrations or tricky logic to implement. For example, you can set this to `run_tests=True`.

::: tip
Please note, when running code tests, you must ensure that the packages that the LLM tries to use within the code it is running are already installed, and errors will be thrown if packages are missing. You can iterate with the outputs, to install any packages it is trying to use by checking its generated `requirements.txt` file and installing any missing packages, or package versions.
:::

### Advanced custom parameters

The L2MAC framework is fully customizable, even with no coding experience needed. As it supports:

* Providing the prompt program explicitly as the argument of `prompt_program`, which can be a path to a prompt program to use or the prompt program as a string in a list format. This is useful for ensuring the task is completed and executed in the exact same, repeatable manner each time, following the input prompt_program, much like how a computer follows a program to execute it re-produce it exactly. It can be perfect for repeatable processes or tasks, for example, based on a given document, processing it into a particular format, or summarizing it. Ideally, this will allow it to automate entire business operations while using custom tools to interact with more complex APIs. We are looking for contributors for this functionality, see [roadmap](../roadmap) for more details.
* Change the prompts that L2MAC uses internally for its operation completely, such as extending L2MAC to a new domain to none of those covered. For example, we change the internal prompts of L2MAC to generate the output for a book for the book domain. However, you can implement a new set of prompts for your desired task. All you have to do is follow the existing format, taking an example of [book.yaml](https://github.com/samholt/L2MAC/blob/master/l2mac/prompts/book.yaml) and adapt it for your use case, and then pass the path to the new file of the prompts as the input to `prompts_file_path`.

Below is an example of producing a great codebase output for a complete coding task.

```python
from l2mac import generate_codebase

codebase: dict = generate_codebase(
    r"""
**Online URL Shortening Service**

**Overview**:
A service that allows users to submit long URLs and then receive a shortened version of that URL for ease of sharing.

**Functional Requirements to implement**:

1. **URL Shortening**:
   - [ ] 1.1. Users can input a URL to be shortened.
   - [ ] 1.2. The system validates that the URL is active and legitimate.
   - [ ] 1.3. The system generates a unique shortened URL.
   - [ ] 1.4. Users can choose custom short links (subject to availability).

2. **Redirection**:
   - [ ] 2.1. Accessing the shortened URL redirects to the original URL.

3. **Analytics**:
   - [ ] 3.1. Users can view statistics about their shortened URLs.
   - [ ] 3.2. View number of clicks.
   - [ ] 3.3. View date/time of each click.
   - [ ] 3.4. View geographical location of the clicker.

4. **User Accounts**:
   - [ ] 4.1. Users can create accounts.
   - [ ] 4.2. Account holders can view all their shortened URLs.
   - [ ] 4.3. Account holders can edit or delete their shortened URLs.
   - [ ] 4.4. Account holders can view analytics for all their shortened URLs.

5. **Admin Dashboard**:
   - [ ] 5.1. Administrators can view all shortened URLs.
   - [ ] 5.2. Administrators can delete any URL or user account.
   - [ ] 5.3. Administrators can monitor system performance and analytics.

6. **Expiration**:
   - [ ] 6.1. Users can set an expiration date/time for the shortened URL.
""",
    steps=20,
    run_tests=True,
)

print(codebase)  # it will print the codebase (repo) complete with all the files as a dictionary
```

<!-- Include Collab link -->